\chapter{Introduction}

Software artifacts evolve throughout their life-cycle---undergoing changes, additions, and deletions. These software modifications often result in errors or unintended behaviors in portions of the software. Therefore, robust unit and integration testing are warranted to maintain integrity and coherence in iterative software development. The safest method to identify bugs, errors, and faults is to run a set of tests that covers the entire software artifact i.e the complete test suite. This practice is referred to as \textit{Retest-all} in the literature~\cite{leung1989insights, wong1997study, onoma1998regression}. Unfortunately, Retest-all is time-consuming and computing-intensive, hence impractical for large-scale repositories. For example, the complete test suite of Apache Hadoop takes approximately 17 hours to complete~\cite{ekstazi}. Furthermore, the resulting test reports from Retest-all are often too large for manual inspection and intervention. For example, Microsoft Windows 8.1 had more than 30 million tests~\cite{herzig2015art}, which made it impractical to tackle all at once. For these reasons, in projects with large test suites it is desirable to perform \emph{regression test selection (RTS)}, i.e., select and run a subset of the complete test suite that assesses only the code that is affected by changes to the software~\cite{memon2017taming, elbaum2014techniques, rothermel2000regression}. 

RTS has been the target of extensive research spanning decades~\cite{rothermel,  leung1989insights, wong1997study, onoma1998regression, rothermel2001prioritizing, yoo2012regression, kimhistory}. Despite an abundance of techniques reported in the literature, this form of software testing is still not prevalent in practice. To a large extent, this lack of adoption occurs because (1) the overhead of selecting a subset of tests can be more expensive than just running the entire test suite~\cite{faulttracer}; (2) the inability of RTS to scale to large programs~\cite{onoma1998regression}; and (3) the risk of missing required tests~\cite{rothermelsaferts}.  

In object-oriented languages, one important dimension of variability among RTS tools is the granularity of test selection. RTS tools can select tests at file-level (all tests in a test file), class-level (all tests in a test class), or method-level (individual unit tests). Method-level RTS is more precise than the coarser variants, and therefore it might seem the most desirable approach to RTS, as it might result in faster test execution time (fewer tests). However, research has shown that the existing method-level RTS techniques are slower than class- and even file-level RTS because of the large overhead involved in tracking dependencies at such fine granularity~\cite{legunsen2016extensive, faulttracer}. This is the problem targeted by our work.

In this thesis, we present TLDR, a static and method-level RTS tool for Java that is both precise and efficient. TLDR simulates the polymorphism principle of OOP, using static analysis. TLDR achieves safety by selecting all tests that correspond to an entity's ~\textit{firewall}, the set of entities which may be impacted given a change to an entity ~\cite{white1992firewall, white2008extended, white2003firewall}. To improve the precision of RTS, TLDR selects tests in method-level like ~\cite{faulttracer}. However, unlike other RTS tools, TLDR leverages a novel multi-threaded ~\textit{pipe-and-filter} architecture \cite{taylor_sa_book}.

Most RTS techniques follow a common sequence of steps: identifying changes in source artifacts (e.g. source code, machine code, jars, etc.), constructing dependency graphs, traversing the graphs to find impacted entities, and finally mapping the impacted entities to one or more test cases in the test suite~\cite{hyrts, ekstazi, gligoric2015ekstazi, b37, starts, faulttracer}. In all existing RTS tools, these steps are performed sequentially. However, the pipeline is such that some of these steps can be executed in parallel. The efficiency of TLDR comes from two aspects of its design. First, selecting tests at the method-level makes it more precise, reducing the number of tests to be retested, as well as the test execution time. This is theoretically true for all method-level RTS tools, but it has been difficult to achieve in practice. Second, and most importantly, parallelizing some steps of the test selection pipeline increases the throughput of the dependency analysis process, making test selection faster. These aspects, together, result in reduced end-to-end testing time for TLDR.

In this thesis, we empirically show that through the adoption of parallelism, method-level RTS can be efficient compared to class-level RTS. In summary, this thesis makes the following novel contributions: 
\begin{itemize}
	\item We propose a multi-threaded pipe-and-filter architecture for RTS tools that reduces test selection overhead.
	\item We construct a precise RTS technique, TDLR, that operates at a finer-granularity (i.e., the method level) and utilizes the proposed pipe-and-filter architecture to achieve major reductions in the time needed to actually execute test cases.
	\item We evaluate TLDR on 20 open source Java projects. We also compare the performance in terms of times with state-of-the-art RTS techniques, Ekstazi, STARTS, HyRTS as well as more conventional testing techniques like retest-all and parallel-retest-all. We show that TLDR incurs similar test selection overhead as STARTS but more than Ekstazi. However, due to its finer granularity, it incurs less test execution time than both STARTS and Ekstazi. Overall, TLDR outperforms both STARTS and Ekstazi, both in terms of the number of selected tests and end-to-end time of testing. 
\end{itemize} 

The remainder of the thesis is organized as follows: In Chapter 2, we discuss the core concept of regression test selection and contemporary RTS tools. In chapter 3, we discuss relevant literature on regression test selection. In chapter 4, we discuss the theoretical concept of our approach to RTS and compare TLDR with other state-of-art RTS tools with an example. In chapter 5, we present our main contributions, i.e., the design and implementation of TLDR. In chapter 6, we discuss the theoretical proof of TLDR's correctness and completeness. In chapter 7, we discuss the empirical evaluation of TLDR. In chapter 8, we discuss the limitations of TLDR and threats to the validity of our evaluation. Finally, in chapter 7, we discuss the future work and concluding remarks.

